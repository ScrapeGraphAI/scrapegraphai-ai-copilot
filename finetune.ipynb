{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1.** Install Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install --no-deps packaging ninja einops flash-attn trl peft accelerate bitsandbytes\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2.** Import Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3.** Login to Your Hugging Face with hf_token. (write access token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 4.** Convert your JSON dataset to Llama3 finetuning format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_user = \"scrapegraphai\"\n",
    "dataset_name = \"scrapegraphcompanion_finetuning_demo\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are ScrapeGraphCompanion, a helpful coding assistant specialized in\n",
    "ScrapeGraphAI, a Python web scraping library that uses language models and\n",
    "direct graph logic. Your role is to assist users in writing code and answering\n",
    "questions related to ScrapeGraphAI, web scraping, and Python programming.\n",
    "\"\"\"\n",
    "\n",
    "class Llama3InstructDataset:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.prompts = []\n",
    "        self.create_prompts()\n",
    "\n",
    "    def create_prompt(self, row):\n",
    "        prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>{row['prompt']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>{row['answer']}<|eot_id|>\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def create_prompts(self):\n",
    "        for row in self.data:\n",
    "            prompt = self.create_prompt(row)\n",
    "            self.prompts.append(prompt)\n",
    "\n",
    "    def get_dataset(self):\n",
    "        df = pd.DataFrame({'prompt': self.prompts})\n",
    "        return df\n",
    "\n",
    "def create_dataset_hf(dataset):\n",
    "    dataset.reset_index(drop=True, inplace=True)\n",
    "    return DatasetDict({\"train\": Dataset.from_pandas(dataset)})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open('/content/dataset.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    dataset = Llama3InstructDataset(data)\n",
    "    df = dataset.get_dataset()\n",
    "\n",
    "    processed_data_path = 'processed_data'\n",
    "    os.makedirs(processed_data_path, exist_ok=True)\n",
    "\n",
    "    llama3_dataset = create_dataset_hf(df)\n",
    "    llama3_dataset.save_to_disk(os.path.join(processed_data_path, \"llama3_dataset\"))\n",
    "    llama3_dataset.push_to_hub(f\"{huggingface_user}/{dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 5.** LoRa Finetuning Configurations\n",
    "\n",
    "- `finetuned_model` sets your models name on HF\n",
    "- `num_train_epochs` sets the number of epochs for training\n",
    "(epoch = 1 pass through your entire dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the configuration for the base model, LoRA and training\n",
    "config = {\n",
    "    \"hugging_face_username\":huggingface_user,\n",
    "    \"model_config\": {\n",
    "        \"base_model\":\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # The base model\n",
    "        \"finetuned_model\":\"llama-3-8b-Instruct-bnb-4bit-aiaustin-demo\", # The finetuned model\n",
    "        \"max_seq_length\": 2048, # The maximum sequence length\n",
    "        \"dtype\":torch.float16, # The data type\n",
    "        \"load_in_4bit\": True, # Load the model in 4-bit\n",
    "    },\n",
    "    \"lora_config\": {\n",
    "      \"r\": 16, # The number of LoRA layers 8, 16, 32, 64\n",
    "      \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"], # The target modules\n",
    "      \"lora_alpha\":16, # The alpha value for LoRA\n",
    "      \"lora_dropout\":0, # The dropout value for LoRA\n",
    "      \"bias\":\"none\", # The bias for LoRA\n",
    "      \"use_gradient_checkpointing\":True, # Use gradient checkpointing\n",
    "      \"use_rslora\":False, # Use RSLora\n",
    "      \"use_dora\":False, # Use DoRa\n",
    "      \"loftq_config\":None # The LoFTQ configuration\n",
    "    },\n",
    "    \"training_dataset\":{\n",
    "        \"name\":f\"{huggingface_user}/{dataset_name}\", # The dataset name(huggingface/datasets)\n",
    "        \"split\":\"train\", # The dataset split\n",
    "        \"input_field\":\"prompt\", # The input field\n",
    "    },\n",
    "    \"training_config\": {\n",
    "        \"per_device_train_batch_size\": 2, # The batch size\n",
    "        \"gradient_accumulation_steps\": 4, # The gradient accumulation steps\n",
    "        \"warmup_steps\": 5, # The warmup steps\n",
    "        \"max_steps\":0, # The maximum steps (0 if the epochs are defined)\n",
    "        \"num_train_epochs\": 10, # The number of training epochs(0 if the maximum steps are defined)\n",
    "        \"learning_rate\": 2e-4, # The learning rate\n",
    "        \"fp16\": torch.cuda.is_bf16_supported(), # The fp16\n",
    "        \"bf16\": False, # The bf16\n",
    "        \"logging_steps\": 1, # The logging steps\n",
    "        \"optim\" :\"adamw_8bit\", # The optimizer\n",
    "        \"weight_decay\" : 0.01,  # The weight decay\n",
    "        \"lr_scheduler_type\": \"linear\", # The learning rate scheduler\n",
    "        \"seed\" : 42, # The seed\n",
    "        \"output_dir\" : \"outputs\", # The output directory\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 6.** Load Llama3-8B, QLoRA & Trainer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model and the tokinizer for the model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = config.get(\"model_config\").get(\"base_model\"),\n",
    "    max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
    "    dtype = config.get(\"model_config\").get(\"dtype\"),\n",
    "    load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n",
    ")\n",
    "\n",
    "# Setup for QLoRA/LoRA peft of the base model\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = config.get(\"lora_config\").get(\"r\"),\n",
    "    target_modules = config.get(\"lora_config\").get(\"target_modules\"),\n",
    "    lora_alpha = config.get(\"lora_config\").get(\"lora_alpha\"),\n",
    "    lora_dropout = config.get(\"lora_config\").get(\"lora_dropout\"),\n",
    "    bias = config.get(\"lora_config\").get(\"bias\"),\n",
    "    use_gradient_checkpointing = config.get(\"lora_config\").get(\"use_gradient_checkpointing\"),\n",
    "    random_state = 42,\n",
    "    use_rslora = config.get(\"lora_config\").get(\"use_rslora\"),\n",
    "    use_dora = config.get(\"lora_config\").get(\"use_dora\"),\n",
    "    loftq_config = config.get(\"lora_config\").get(\"loftq_config\"),\n",
    ")\n",
    "\n",
    "# Loading the training dataset\n",
    "dataset_train = load_dataset(config.get(\"training_dataset\").get(\"name\"), split = config.get(\"training_dataset\").get(\"split\"))\n",
    "\n",
    "# Setting up the trainer for the model\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset_train,\n",
    "    dataset_text_field = config.get(\"training_dataset\").get(\"input_field\"),\n",
    "    max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = config.get(\"training_config\").get(\"per_device_train_batch_size\"),\n",
    "        gradient_accumulation_steps = config.get(\"training_config\").get(\"gradient_accumulation_steps\"),\n",
    "        warmup_steps = config.get(\"training_config\").get(\"warmup_steps\"),\n",
    "        max_steps = config.get(\"training_config\").get(\"max_steps\"),\n",
    "        num_train_epochs= config.get(\"training_config\").get(\"num_train_epochs\"),\n",
    "        learning_rate = config.get(\"training_config\").get(\"learning_rate\"),\n",
    "        fp16 = config.get(\"training_config\").get(\"fp16\"),\n",
    "        bf16 = False,\n",
    "        logging_steps = config.get(\"training_config\").get(\"logging_steps\"),\n",
    "        optim = config.get(\"training_config\").get(\"optim\"),\n",
    "        weight_decay = config.get(\"training_config\").get(\"weight_decay\"),\n",
    "        lr_scheduler_type = config.get(\"training_config\").get(\"lr_scheduler_type\"),\n",
    "        seed = 42,\n",
    "        output_dir = config.get(\"training_config\").get(\"output_dir\"),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 7.** Train Your Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 8.** Save Trainer Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trainer_stats.json\", \"w\") as f:\n",
    "    json.dump(trainer_stats, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 9.** Save Finetuned Model & Push to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"q4_k_m\")\n",
    "model.push_to_hub_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"q4_k_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 10.** Test your pretrained model in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the fine-tuned model and the tokenizer for inference\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = config.get(\"model_config\").get(\"finetuned_model\"),\n",
    "        max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
    "        dtype = config.get(\"model_config\").get(\"dtype\"),\n",
    "        load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n",
    "    )\n",
    "\n",
    "# Using FastLanguageModel for fast inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Tokenizing the input and generating the output\n",
    "prompt = input('TYPE PROMPT TO LLAMA3: ')\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    f\"<|start_header_id|>system<|end_header_id|>{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>{prompt}<|end_header_id|>\"\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdec2f261ad5d236aea1ce5a7c81a58266dc97be97ef29e76e60107890463708"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
