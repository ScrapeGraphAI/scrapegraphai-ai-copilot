[
    {
        "prompt": "how is generate_answer_csv_node implemented in Scrapegraphai?",
        "answer": "\"\"\"\ngg\nModule for generating the answer node\n\"\"\"\n\nfrom typing import List, Optional\nfrom langchain.prompts import PromptTemplate\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_core.runnables import RunnableParallel\nfrom tqdm import tqdm\nfrom ..utils.logging import get_logger\nfrom .base_node import BaseNode\nfrom ..helpers.generate_answer_node_csv_prompts import template_chunks_csv, template_no_chunks_csv, template_merge_csv\n\n\nclass GenerateAnswerCSVNode(BaseNode):\n    \"\"\"\n    A node that generates an answer using a language model (LLM) based on the user's input\n    and the content extracted from a webpage. It constructs a prompt from the user's input\n    and the scraped content, feeds it to the LLM, and parses the LLM's response to produce\n    an answer.\n\n    Attributes:\n        llm_model: An instance of a language model client, configured for generating answers.\n        node_name (str): The unique identifier name for the node, defaulting\n        to \"GenerateAnswerNodeCsv\".\n        node_type (str): The type of the node, set to \"node\" indicating a\n        standard operational node.\n\n    Args:\n        llm_model: An instance of the language model client (e.g., ChatOpenAI) used\n        for generating answers.\n        node_name (str, optional): The unique identifier name for the node.\n        Defaults to \"GenerateAnswerNodeCsv\".\n\n    Methods:\n        execute(state): Processes the input and document from the state to generate an answer,\n                        updating the state with the generated answer under the 'answer' key.\n    \"\"\"\n\n    def __init__(\n        self,\n        input: str,\n        output: List[str],\n        node_config: Optional[dict] = None,\n        node_name: str = \"GenerateAnswerCSV\",\n    ):\n        \"\"\"\n        Initializes the GenerateAnswerNodeCsv with a language model client and a node name.\n        Args:\n            llm_model: An instance of the OpenAIImageToText class.\n            node_name (str): name of the node\n        \"\"\"\n        super().__init__(node_name, \"node\", input, output, 2, node_config)\n\n        self.llm_model = node_config[\"llm_model\"]\n\n        self.verbose = (\n            False if node_config is None else node_config.get(\"verbose\", False)\n        )\n        \n        self.additional_info = node_config.get(\"additional_info\")\n\n    def execute(self, state):\n        \"\"\"\n        Generates an answer by constructing a prompt from the user's input and the scraped\n        content, querying the language model, and parsing its response.\n\n        The method updates the state with the generated answer under the 'answer' key.\n\n        Args:\n            state (dict): The current state of the graph, expected to contain 'user_input',\n                          and optionally 'parsed_document' or 'relevant_chunks' within 'keys'.\n\n        Returns:\n            dict: The updated state with the 'answer' key containing the generated answer.\n\n        Raises:\n            KeyError: If 'user_input' or 'document' is not found in the state, indicating\n                      that the necessary information for generating an answer is missing.\n        \"\"\"\n\n        self.logger.info(f\"--- Executing {self.node_name} Node ---\")\n\n        # Interpret input keys based on the provided input expression\n        input_keys = self.get_input_keys(state)\n\n        # Fetching data from the state based on the input keys\n        input_data = [state[key] for key in input_keys]\n\n        user_prompt = input_data[0]\n        doc = input_data[1]\n\n        # Initialize the output parser\n        if self.node_config.get(\"schema\", None) is not None:\n            output_parser = JsonOutputParser(pydantic_object=self.node_config[\"schema\"])\n        else:\n            output_parser = JsonOutputParser()\n\n        template_no_chunks_csv_prompt = template_no_chunks_csv\n        template_chunks_csv_prompt = template_chunks_csv\n        template_merge_csv_prompt  = template_merge_csv\n\n        if self.additional_info is not None:\n            template_no_chunks_csv_prompt = self.additional_info + template_no_chunks_csv\n            template_chunks_csv_prompt = self.additional_info + template_chunks_csv\n            template_merge_csv_prompt = self.additional_info + template_merge_csv\n\n        format_instructions = output_parser.get_format_instructions()\n\n        chains_dict = {}\n\n        if len(doc) == 1:\n            prompt = PromptTemplate(\n                template=template_no_chunks_csv_prompt,\n                input_variables=[\"question\"],\n                partial_variables={\n                    \"context\": doc,\n                    \"format_instructions\": format_instructions,\n                },\n            )\n\n            chain =  prompt | self.llm_model | output_parser\n            answer = chain.invoke({\"question\": user_prompt})\n            state.update({self.output[0]: answer})\n            return state\n\n        for i, chunk in enumerate(\n            tqdm(doc, desc=\"Processing chunks\", disable=not self.verbose)\n        ):\n            prompt = PromptTemplate(\n                    template=template_chunks_csv_prompt,\n                    input_variables=[\"question\"],\n                    partial_variables={\n                        \"context\": chunk,\n                        \"chunk_id\": i + 1,\n                        \"format_instructions\": format_instructions,\n                    },\n                )\n\n            chain_name = f\"chunk{i+1}\"\n            chains_dict[chain_name] = prompt | self.llm_model | output_parser\n\n        async_runner = RunnableParallel(**chains_dict)\n\n        batch_results =  async_runner.invoke({\"question\": user_prompt})\n\n        merge_prompt = PromptTemplate(\n                template = template_merge_csv_prompt,\n                input_variables=[\"context\", \"question\"],\n                partial_variables={\"format_instructions\": format_instructions},\n            )\n\n        merge_chain = merge_prompt | self.llm_model | output_parser\n        answer = merge_chain.invoke({\"context\": batch_results, \"question\": user_prompt})\n\n        state.update({self.output[0]: answer})\n        return state"
    }
]