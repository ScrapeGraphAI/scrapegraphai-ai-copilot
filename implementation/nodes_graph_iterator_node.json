[
    {
        "prompt": "how is graph_iterator_node implemented in Scrapegraphai?",
        "answer": "\"\"\"\nGraphIterator Module\n\"\"\"\n\nimport asyncio\nimport copy\nfrom typing import List, Optional\n\nfrom tqdm.asyncio import tqdm\n\nfrom ..utils.logging import get_logger\nfrom .base_node import BaseNode\n\n_default_batchsize = 16\n\n\nclass GraphIteratorNode(BaseNode):\n    \"\"\"\n    A node responsible for instantiating and running multiple graph instances in parallel.\n    It creates as many graph instances as the number of elements in the input list.\n\n    Attributes:\n        verbose (bool): A flag indicating whether to show print statements during execution.\n\n    Args:\n        input (str): Boolean expression defining the input keys needed from the state.\n        output (List[str]): List of output keys to be updated in the state.\n        node_config (dict): Additional configuration for the node.\n        node_name (str): The unique identifier name for the node, defaulting to \"Parse\".\n    \"\"\"\n\n    def __init__(\n        self,\n        input: str,\n        output: List[str],\n        node_config: Optional[dict] = None,\n        node_name: str = \"GraphIterator\",\n    ):\n        super().__init__(node_name, \"node\", input, output, 2, node_config)\n\n        self.verbose = (\n            False if node_config is None else node_config.get(\"verbose\", False)\n        )\n\n    def execute(self, state: dict) -> dict:\n        \"\"\"\n        Executes the node's logic to instantiate and run multiple graph instances in parallel.\n\n        Args:\n            state (dict): The current state of the graph. The input keys will be used to fetch\n                            the correct data from the state.\n\n        Returns:\n            dict: The updated state with the output key containing the results of the graph instances.\n\n        Raises:\n            KeyError: If the input keys are not found in the state, indicating that the\n                        necessary information for running the graph instances is missing.\n        \"\"\"\n        batchsize = self.node_config.get(\"batchsize\", _default_batchsize)\n\n        self.logger.info(\n            f\"--- Executing {self.node_name} Node with batchsize {batchsize} ---\"\n        )\n\n        try:\n            eventloop = asyncio.get_event_loop()\n        except RuntimeError:\n            eventloop = None\n\n        if eventloop and eventloop.is_running():\n            state = eventloop.run_until_complete(self._async_execute(state, batchsize))\n        else:\n            state = asyncio.run(self._async_execute(state, batchsize))\n\n        return state\n\n    async def _async_execute(self, state: dict, batchsize: int) -> dict:\n        \"\"\"asynchronously executes the node's logic with multiple graph instances\n        running in parallel, using a semaphore of some size for concurrency regulation\n\n        Args:\n            state: The current state of the graph.\n            batchsize: The maximum number of concurrent instances allowed.\n\n        Returns:\n            The updated state with the output key containing the results\n            aggregated out of all parallel graph instances.\n\n        Raises:\n            KeyError: If the input keys are not found in the state.\n        \"\"\"\n\n        # interprets input keys based on the provided input expression\n        input_keys = self.get_input_keys(state)\n\n        # fetches data from the state based on the input keys\n        input_data = [state[key] for key in input_keys]\n\n        user_prompt = input_data[0]\n        urls = input_data[1]\n\n        graph_instance = self.node_config.get(\"graph_instance\", None)\n\n        if graph_instance is None:\n            raise ValueError(\"graph instance is required for concurrent execution\")\n\n        # Assign depth level to the graph\n        if \"graph_depth\" in graph_instance.config:\n            graph_instance.config[\"graph_depth\"] += 1\n        else:\n            graph_instance.config[\"graph_depth\"] = 1\n\n        graph_instance.prompt = user_prompt\n\n        participants = []\n\n        # semaphore to limit the number of concurrent tasks\n        semaphore = asyncio.Semaphore(batchsize)\n\n        async def _async_run(graph):\n            async with semaphore:\n                return await asyncio.to_thread(graph.run)\n\n        # creates a deepcopy of the graph instance for each endpoint\n        for url in urls:\n            instance = copy.copy(graph_instance)\n            instance.source = url\n\n            participants.append(instance)\n\n        futures = [_async_run(graph) for graph in participants]\n\n        answers = await tqdm.gather(\n            *futures, desc=\"processing graph instances\", disable=not self.verbose\n        )\n\n        state.update({self.output[0]: answers})\n\n        return state\n"
    }
]