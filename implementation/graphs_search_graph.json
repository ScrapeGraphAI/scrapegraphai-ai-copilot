[
    {
        "prompt": "how is search_graph implemented in Scrapegraphai?",
        "answer": "\"\"\" \nSearchGraph Module\n\"\"\"\n\nfrom copy import copy, deepcopy\nfrom typing import Optional, List\nfrom pydantic import BaseModel\n\nfrom .base_graph import BaseGraph\nfrom .abstract_graph import AbstractGraph\nfrom .smart_scraper_graph import SmartScraperGraph\n\nfrom ..nodes import (\n    SearchInternetNode,\n    GraphIteratorNode,\n    MergeAnswersNode\n)\n\n\n\nclass SearchGraph(AbstractGraph):\n    \"\"\" \n    SearchGraph is a scraping pipeline that searches the internet for answers to a given prompt.\n    It only requires a user prompt to search the internet and generate an answer.\n\n    Attributes:\n        prompt (str): The user prompt to search the internet.\n        llm_model (dict): The configuration for the language model.\n        embedder_model (dict): The configuration for the embedder model.\n        headless (bool): A flag to run the browser in headless mode.\n        verbose (bool): A flag to display the execution information.\n        model_token (int): The token limit for the language model.\n        considered_urls (List[str]): A list of URLs considered during the search.\n\n    Args:\n        prompt (str): The user prompt to search the internet.\n        config (dict): Configuration parameters for the graph.\n        schema (Optional[BaseModel]): The schema for the graph output.\n\n    Example:\n        >>> search_graph = SearchGraph(\n        ...     \"What is Chioggia famous for?\",\n        ...     {\"llm\": {\"model\": \"gpt-3.5-turbo\"}}\n        ... )\n        >>> result = search_graph.run()\n        >>> print(search_graph.get_considered_urls())\n    \"\"\"\n\n    def __init__(self, prompt: str, config: dict, schema: Optional[BaseModel] = None):\n        self.max_results = config.get(\"max_results\", 3)\n\n        if all(isinstance(value, str) for value in config.values()):\n            self.copy_config = copy(config)\n        else:\n            self.copy_config = deepcopy(config)\n        self.copy_schema = deepcopy(schema)\n        self.considered_urls = []  # New attribute to store URLs\n\n        super().__init__(prompt, config, schema)\n\n    def _create_graph(self) -> BaseGraph:\n        \"\"\"\n        Creates the graph of nodes representing the workflow for web scraping and searching.\n\n        Returns:\n            BaseGraph: A graph instance representing the web scraping and searching workflow.\n        \"\"\"\n\n        # Create a SmartScraperGraph instance\n        smart_scraper_instance = SmartScraperGraph(\n            prompt=\"\",\n            source=\"\",\n            config=self.copy_config,\n            schema=self.copy_schema\n        )\n\n        # Define the graph nodes\n        search_internet_node = SearchInternetNode(\n            input=\"user_prompt\",\n            output=[\"urls\"],\n            node_config={\n                \"llm_model\": self.llm_model,\n                \"max_results\": self.max_results\n            }\n        )\n        graph_iterator_node = GraphIteratorNode(\n            input=\"user_prompt & urls\",\n            output=[\"results\"],\n            node_config={\n                \"graph_instance\": smart_scraper_instance,\n            }\n        )\n\n        merge_answers_node = MergeAnswersNode(\n            input=\"user_prompt & results\",\n            output=[\"answer\"],\n            node_config={\n                \"llm_model\": self.llm_model,\n                \"schema\": self.schema\n            }\n        )\n\n        return BaseGraph(\n            nodes=[\n                search_internet_node,\n                graph_iterator_node,\n                merge_answers_node\n            ],\n            edges=[\n                (search_internet_node, graph_iterator_node),\n                (graph_iterator_node, merge_answers_node)\n            ],\n            entry_point=search_internet_node,\n            graph_name=self.__class__.__name__\n        )\n\n    def run(self) -> str:\n        \"\"\"\n        Executes the web scraping and searching process.\n\n        Returns:\n            str: The answer to the prompt.\n        \"\"\"\n        inputs = {\"user_prompt\": self.prompt}\n        self.final_state, self.execution_info = self.graph.execute(inputs)\n\n        # Store the URLs after execution\n        if 'urls' in self.final_state:\n            self.considered_urls = self.final_state['urls']\n\n        return self.final_state.get(\"answer\", \"No answer found.\")\n\n    def get_considered_urls(self) -> List[str]:\n        \"\"\"\n        Returns the list of URLs considered during the search.\n\n        Returns:\n            List[str]: A list of URLs considered during the search.\n        \"\"\"\n        return self.considered_urls\n"
    }
]