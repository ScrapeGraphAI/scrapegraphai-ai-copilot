[
    {
        "prompt": "how is pdf_scraper_multi_graph implemented in Scrapegraphai?",
        "answer": "\"\"\" \nPdfScraperMultiGraph Module\n\"\"\"\n\nfrom copy import copy, deepcopy\nfrom typing import List, Optional\nfrom pydantic import BaseModel\n\nfrom .base_graph import BaseGraph\nfrom .abstract_graph import AbstractGraph\nfrom .pdf_scraper_graph import PDFScraperGraph\n\nfrom ..nodes import (\n    GraphIteratorNode,\n    MergeAnswersNode\n)\n\n\nclass PdfScraperMultiGraph(AbstractGraph):\n    \"\"\" \n    PdfScraperMultiGraph is a scraping pipeline that scrapes a \n    list of URLs and generates answers to a given prompt.\n    It only requires a user prompt and a list of URLs.\n\n    Attributes:\n        prompt (str): The user prompt to search the internet.\n        llm_model (dict): The configuration for the language model.\n        embedder_model (dict): The configuration for the embedder model.\n        headless (bool): A flag to run the browser in headless mode.\n        verbose (bool): A flag to display the execution information.\n        model_token (int): The token limit for the language model.\n\n    Args:\n        prompt (str): The user prompt to search the internet.\n        source (List[str]): The source of the graph.\n        config (dict): Configuration parameters for the graph.\n        schema (Optional[BaseModel]): The schema for the graph output.\n\n    Example:\n        >>> search_graph = MultipleSearchGraph(\n        ...     \"What is Chioggia famous for?\",\n        ...     {\"llm\": {\"model\": \"gpt-3.5-turbo\"}}\n        ... )\n        >>> result = search_graph.run()\n    \"\"\"\n\n    def __init__(self, prompt: str, source: List[str], config: dict, schema: Optional[BaseModel] = None):\n\n        if all(isinstance(value, str) for value in config.values()):\n            self.copy_config = copy(config)\n        else:\n            self.copy_config = deepcopy(config)\n\n        self.copy_schema = deepcopy(schema)\n\n        super().__init__(prompt, config, source, schema)\n\n    def _create_graph(self) -> BaseGraph:\n        \"\"\"\n        Creates the graph of nodes representing the workflow for web scraping and searching.\n\n        Returns:\n            BaseGraph: A graph instance representing the web scraping and searching workflow.\n        \"\"\"\n\n        # ************************************************\n        # Create a PDFScraperGraph instance\n        # ************************************************\n\n        pdf_scraper_instance = PDFScraperGraph(\n            prompt=\"\",\n            source=\"\",\n            config=self.copy_config,\n            schema=self.copy_schema\n        )\n\n        # ************************************************\n        # Define the graph nodes\n        # ************************************************\n\n        graph_iterator_node = GraphIteratorNode(\n            input=\"user_prompt & pdfs\",\n            output=[\"results\"],\n            node_config={\n                \"graph_instance\": pdf_scraper_instance,\n            }\n        )\n\n        merge_answers_node = MergeAnswersNode(\n            input=\"user_prompt & results\",\n            output=[\"answer\"],\n            node_config={\n                \"llm_model\": self.llm_model,\n                \"schema\": self.schema\n            }\n        )\n\n        return BaseGraph(\n            nodes=[\n                graph_iterator_node,\n                merge_answers_node,\n            ],\n            edges=[\n                (graph_iterator_node, merge_answers_node),\n            ],\n            entry_point=graph_iterator_node,\n            graph_name=self.__class__.__name__\n        )\n\n    def run(self) -> str:\n        \"\"\"\n        Executes the web scraping and searching process.\n\n        Returns:\n            str: The answer to the prompt.\n        \"\"\"\n        inputs = {\"user_prompt\": self.prompt, \"pdfs\": self.source}\n        self.final_state, self.execution_info = self.graph.execute(inputs)\n\n        return self.final_state.get(\"answer\", \"No answer found.\")\n"
    }
]