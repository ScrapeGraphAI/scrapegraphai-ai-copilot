[
    {
        "prompt": "how is search_internet_node implemented in Scrapegraphai?",
        "answer": "\"\"\"\nSearchInternetNode Module\n\"\"\"\nfrom typing import List, Optional\nfrom langchain.output_parsers import CommaSeparatedListOutputParser\nfrom langchain.prompts import PromptTemplate\nfrom langchain_community.chat_models import ChatOllama\nfrom ..utils.logging import get_logger\nfrom ..utils.research_web import search_on_web\nfrom .base_node import BaseNode\n\nclass SearchInternetNode(BaseNode):\n    \"\"\"\n    A node that generates a search query based on the user's input and searches the internet\n    for relevant information. The node constructs a prompt for the language model, submits it,\n    and processes the output to generate a search query. It then uses the search query to find\n    relevant information on the internet and updates the state with the generated answer.\n\n    Attributes:\n        llm_model: An instance of the language model client used for generating search queries.\n        verbose (bool): A flag indicating whether to show print statements during execution.\n\n    Args:\n        input (str): Boolean expression defining the input keys needed from the state.\n        output (List[str]): List of output keys to be updated in the state.\n        node_config (dict): Additional configuration for the node.\n        node_name (str): The unique identifier name for the node, defaulting to \"SearchInternet\".\n    \"\"\"\n\n    def __init__(\n        self,\n        input: str,\n        output: List[str],\n        node_config: Optional[dict] = None,\n        node_name: str = \"SearchInternet\",\n    ):\n        super().__init__(node_name, \"node\", input, output, 1, node_config)\n\n        self.llm_model = node_config[\"llm_model\"]\n        self.verbose = (\n            False if node_config is None else node_config.get(\"verbose\", False)\n        )\n        self.search_engine = node_config.get(\"search_engine\", \"google\")\n        self.max_results = node_config.get(\"max_results\", 3)\n\n    def execute(self, state: dict) -> dict:\n        \"\"\"\n        Generates an answer by constructing a prompt from the user's input and the scraped\n        content, querying the language model, and parsing its response.\n\n        The method updates the state with the generated answer.\n\n        Args:\n            state (dict): The current state of the graph. The input keys will be used to fetch the\n                            correct data types from the state.\n\n        Returns:\n            dict: The updated state with the output key containing the generated answer.\n\n        Raises:\n            KeyError: If the input keys are not found in the state, indicating that the\n                        necessary information for generating the answer is missing.\n        \"\"\"\n\n        self.logger.info(f\"--- Executing {self.node_name} Node ---\")\n\n        input_keys = self.get_input_keys(state)\n\n        # Fetching data from the state based on the input keys\n        input_data = [state[key] for key in input_keys]\n\n        user_prompt = input_data[0]\n\n        output_parser = CommaSeparatedListOutputParser()\n\n        search_template = \"\"\"\n        PROMPT:\n        You are a search engine and you need to generate a search query based on the user's prompt. \\n\n        Given the following user prompt, return a query that can be \n        used to search the internet for relevant information. \\n\n        You should return only the query string without any additional sentences. \\n\n        For example, if the user prompt is \"What is the capital of France?\",\n        you should return \"capital of France\". \\n\n        If you return something else, you will get a really bad grade. \\n\n        USER PROMPT: {user_prompt}\"\"\"\n\n        search_prompt = PromptTemplate(\n            template=search_template,\n            input_variables=[\"user_prompt\"],\n        )\n\n        # Execute the chain to get the search query\n        search_answer = search_prompt | self.llm_model | output_parser\n        \n        # Ollama: Use no json format when creating the search query\n        if isinstance(self.llm_model, ChatOllama) and self.llm_model.format == 'json':\n            self.llm_model.format = None\n            search_query = search_answer.invoke({\"user_prompt\": user_prompt})[0]\n            self.llm_model.format = 'json'\n        else:\n            search_query = search_answer.invoke({\"user_prompt\": user_prompt})[0]\n\n        self.logger.info(f\"Search Query: {search_query}\")\n\n        answer = search_on_web(query=search_query, max_results=self.max_results,\n                               search_engine=self.search_engine)\n\n        if len(answer) == 0:\n            # raise an exception if no answer is found\n            raise ValueError(\"Zero results found for the search query.\")\n\n        # Update the state with the generated answer\n        state.update({self.output[0]: answer})\n        return state\n"
    }
]