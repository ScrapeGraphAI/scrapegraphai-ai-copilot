[
    {
        "prompt": "how is implemented token_calculator",
        "answer": "\"\"\" \nModule for truncatinh in chunks the messages\n\"\"\"\nfrom typing import List\nimport tiktoken\nfrom ..helpers.models_tokens import models_tokens\n\n\ndef truncate_text_tokens(text: str, model: str, encoding_name: str) -> List[str]:\n    \"\"\"\n    Truncates text into chunks that are small enough to be processed by specified llm models.\n\n    Args:\n        text (str): The input text to be truncated.\n        model (str): The name of the llm model to determine the maximum token limit.\n        encoding_name (str): The encoding strategy used to encode the text before truncation.\n\n    Returns:\n        List[str]: A list of text chunks, each within the token limit of the specified model.\n\n    Example:\n        >>> truncate_text_tokens(\"This is a sample text for truncation.\", \"GPT-3\", \"EMBEDDING_ENCODING\")\n        [\"This is a sample text\", \"for truncation.\"]\n\n    This function ensures that each chunk of text can be tokenized by the specified model without exceeding the model's token limit.\n    \"\"\"\n\n    encoding = tiktoken.get_encoding(encoding_name)\n    max_tokens = models_tokens[model] - 500\n    encoded_text = encoding.encode(text)\n\n    chunks = [encoded_text[i:i + max_tokens]\n              for i in range(0, len(encoded_text), max_tokens)]\n\n    result = [encoding.decode(chunk) for chunk in chunks]\n\n    return result\n"
    }
]