[
    {
        "prompt": "how is search_link_graph implemented in Scrapegraphai?",
        "answer": "\"\"\" SearchLinkGraph Module \"\"\"\nfrom typing import Optional\nimport logging\nfrom pydantic import BaseModel\nfrom .base_graph import BaseGraph\nfrom .abstract_graph import AbstractGraph\n\n\nfrom ..nodes import ( FetchNode, ParseNode, SearchLinkNode )\n\nclass SearchLinkGraph(AbstractGraph): \n    \"\"\" \n    SearchLinkGraph is a scraping pipeline that automates the process of extracting information from web pages using a natural language model to interpret and answer prompts.\n\n    Attributes:\n        prompt (str): The prompt for the graph.\n        source (str): The source of the graph.\n        config (dict): Configuration parameters for the graph.\n        schema (BaseModel): The schema for the graph output.\n        llm_model: An instance of a language model client, configured for generating answers.\n        embedder_model: An instance of an embedding model client, \n        configured for generating embeddings.\n        verbose (bool): A flag indicating whether to show print statements during execution.\n        headless (bool): A flag indicating whether to run the graph in headless mode.\n\n    Args:\n        source (str): The source of the graph.\n        config (dict): Configuration parameters for the graph.\n        schema (BaseModel, optional): The schema for the graph output. Defaults to None.\n\n    Example:\n        >>> smart_scraper = SearchLinkGraph(\n        ...     \"List me all the attractions in Chioggia.\",\n        ...     \"https://en.wikipedia.org/wiki/Chioggia\",\n        ...     {\"llm\": {\"model\": \"gpt-3.5-turbo\"}}\n        ... )\n        >>> result = smart_scraper.run()\n    \"\"\"\n\n    def __init__(self, source: str, config: dict, schema: Optional[BaseModel] = None):\n        super().__init__(\"\", config, source, schema)\n\n        self.input_key = \"url\" if source.startswith(\"http\") else \"local_dir\"\n\n    def _create_graph(self) -> BaseGraph:\n        \"\"\"\n        Creates the graph of nodes representing the workflow for web scraping.\n\n        Returns:\n            BaseGraph: A graph instance representing the web scraping workflow.\n        \"\"\"\n\n        fetch_node = FetchNode(\n            input=\"url| local_dir\",\n            output=[\"doc\", \"link_urls\", \"img_urls\"],\n            node_config={\n                \"llm_model\": self.llm_model,\n                \"force\": self.config.get(\"force\", False),\n                \"cut\": self.config.get(\"cut\", True),\n                \"loader_kwargs\": self.config.get(\"loader_kwargs\", {}),\n            }\n        )\n        parse_node = ParseNode(\n            input=\"doc\",\n            output=[\"parsed_doc\"],\n            node_config={\n                \"chunk_size\": self.model_token\n            }\n        )\n        search_link_node = SearchLinkNode(\n            input=\"doc\",\n            output=[\"parsed_doc\"],\n            node_config={\n                \"llm_model\": self.llm_model,\n                \"chunk_size\": self.model_token\n            }\n        )\n\n        return BaseGraph(\n            nodes=[\n                fetch_node,\n                parse_node,\n                search_link_node\n            ],\n            edges=[\n                (fetch_node, parse_node),\n                (parse_node, search_link_node)\n            ],\n            entry_point=fetch_node,\n            graph_name=self.__class__.__name__\n        )\n\n    def run(self) -> str:\n        \"\"\"\n        Executes the scraping process and returns the answer to the prompt.\n\n        Returns:\n            str: The answer to the prompt.\n        \"\"\"\n\n        inputs = {\"user_prompt\": self.prompt, self.input_key: self.source}\n        self.final_state, self.execution_info = self.graph.execute(inputs)\n\n        return self.final_state.get(\"parsed_doc\", \"No answer found.\")"
    }
]