[
    {
        "prompt": "how is abstract_graph implemented in Scrapegraphai?",
        "answer": "\"\"\"\nAbstractGraph Module\n\"\"\"\n\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\nimport uuid\nfrom pydantic import BaseModel\n\nfrom langchain_community.chat_models import ErnieBotChat\nfrom langchain_nvidia_ai_endpoints import ChatNVIDIA\nfrom langchain.chat_models import init_chat_model\n\nfrom ..helpers import models_tokens\nfrom ..models import (\n    OneApi,\n    DeepSeek\n)\nfrom ..utils.logging import set_verbosity_warning, set_verbosity_info\n\n\n\nclass AbstractGraph(ABC):\n    \"\"\"\n    Scaffolding class for creating a graph representation and executing it.\n\n        prompt (str): The prompt for the graph.\n        source (str): The source of the graph.\n        config (dict): Configuration parameters for the graph.\n        schema (BaseModel): The schema for the graph output.\n        llm_model: An instance of a language model client, configured for generating answers.\n        verbose (bool): A flag indicating whether to show print statements during execution.\n        headless (bool): A flag indicating whether to run the graph in headless mode.\n\n    Args:\n        prompt (str): The prompt for the graph.\n        config (dict): Configuration parameters for the graph.\n        source (str, optional): The source of the graph.\n        schema (str, optional): The schema for the graph output.\n\n    Example:\n        >>> class MyGraph(AbstractGraph):\n        ...     def _create_graph(self):\n        ...         # Implementation of graph creation here\n        ...         return graph\n        ...\n        >>> my_graph = MyGraph(\"Example Graph\", \n        {\"llm\": {\"model\": \"gpt-3.5-turbo\"}}, \"example_source\")\n        >>> result = my_graph.run()\n    \"\"\"\n\n    def __init__(self, prompt: str, config: dict,\n                 source: Optional[str] = None, schema: Optional[BaseModel] = None):\n\n        self.prompt = prompt\n        self.source = source\n        self.config = config\n        self.schema = schema\n        self.llm_model = self._create_llm(config[\"llm\"])\n        self.verbose = False if config is None else config.get(\n            \"verbose\", False)\n        self.headless = True if self.config is None else config.get(\n            \"headless\", True)\n        self.loader_kwargs = self.config.get(\"loader_kwargs\", {})\n        self.cache_path = self.config.get(\"cache_path\", False)\n        self.browser_base = self.config.get(\"browser_base\")\n\n        # Create the graph\n        self.graph = self._create_graph()\n        self.final_state = None\n        self.execution_info = None\n\n        # Set common configuration parameters\n\n        verbose = bool(config and config.get(\"verbose\"))\n\n        if verbose:\n            set_verbosity_info()\n        else:\n            set_verbosity_warning()\n\n        common_params = {\n            \"headless\": self.headless,\n            \"verbose\": self.verbose,\n            \"loader_kwargs\": self.loader_kwargs,\n            \"llm_model\": self.llm_model,\n            \"cache_path\": self.cache_path,\n            }\n\n        self.set_common_params(common_params, overwrite=True)\n\n        # set burr config\n        self.burr_kwargs = config.get(\"burr_kwargs\", None)\n        if self.burr_kwargs is not None:\n            self.graph.use_burr = True\n            if \"app_instance_id\" not in self.burr_kwargs:\n                # set a random uuid for the app_instance_id to avoid conflicts\n                self.burr_kwargs[\"app_instance_id\"] = str(uuid.uuid4())\n\n            self.graph.burr_config = self.burr_kwargs\n\n    def set_common_params(self, params: dict, overwrite=False):\n        \"\"\"\n        Pass parameters to every node in the graph unless otherwise defined in the graph.\n\n        Args:\n            params (dict): Common parameters and their values.\n        \"\"\"\n\n        for node in self.graph.nodes:\n            node.update_config(params, overwrite)\n\n    def _create_llm(self, llm_config: dict) -> object:\n        \"\"\"\n        Create a large language model instance based on the configuration provided.\n\n        Args:\n            llm_config (dict): Configuration parameters for the language model.\n\n        Returns:\n            object: An instance of the language model client.\n\n        Raises:\n            KeyError: If the model is not supported.\n        \"\"\"\n\n        llm_defaults = {\"temperature\": 0, \"streaming\": False}\n        llm_params = {**llm_defaults, **llm_config}\n\n        # If model instance is passed directly instead of the model details\n        if \"model_instance\" in llm_params:\n            try:\n                self.model_token = llm_params[\"model_tokens\"]\n            except KeyError as exc:\n                raise KeyError(\"model_tokens not specified\") from exc\n            return llm_params[\"model_instance\"]\n\n        # Instantiate the language model based on the model name (models that use the common interface)\n        def handle_model(model_name, provider, token_key, default_token=8192):\n            try:\n                self.model_token = models_tokens[provider][token_key]\n            except KeyError:\n                print(f\"Model not found, using default token size ({default_token})\")\n                self.model_token = default_token\n            llm_params[\"model_provider\"] = provider\n            llm_params[\"model\"] = model_name\n            return init_chat_model(**llm_params)\n\n        if \"azure\" in llm_params[\"model\"]:\n            model_name = llm_params[\"model\"].split(\"/\")[-1]\n            return handle_model(model_name, \"azure_openai\", model_name)\n\n        if \"gpt-\" in llm_params[\"model\"]:\n            return handle_model(llm_params[\"model\"], \"openai\", llm_params[\"model\"])\n\n        if \"fireworks\" in llm_params[\"model\"]:\n            model_name = \"/\".join(llm_params[\"model\"].split(\"/\")[1:])\n            token_key = llm_params[\"model\"].split(\"/\")[-1]\n            return handle_model(model_name, \"fireworks\", token_key)\n\n        if \"gemini\" in llm_params[\"model\"]:\n            model_name = llm_params[\"model\"].split(\"/\")[-1]\n            return handle_model(model_name, \"google_genai\", model_name)\n\n        if llm_params[\"model\"].startswith(\"claude\"):\n            model_name = llm_params[\"model\"].split(\"/\")[-1]\n            return handle_model(model_name, \"anthropic\", model_name)\n\n        if llm_params[\"model\"].startswith(\"vertexai\"):\n            return handle_model(llm_params[\"model\"], \"google_vertexai\", llm_params[\"model\"])\n\n        if \"ollama\" in llm_params[\"model\"]:\n            model_name = llm_params[\"model\"].split(\"ollama/\")[-1]\n            token_key = model_name if \"model_tokens\" not in llm_params else llm_params[\"model_tokens\"]\n            return handle_model(model_name, \"ollama\", token_key)\n\n        if \"hugging_face\" in llm_params[\"model\"]:\n            model_name = llm_params[\"model\"].split(\"/\")[-1]\n            return handle_model(model_name, \"hugging_face\", model_name)\n\n        if \"groq\" in llm_params[\"model\"]:\n            model_name = llm_params[\"model\"].split(\"/\")[-1]\n            return handle_model(model_name, \"groq\", model_name)\n\n        if \"bedrock\" in llm_params[\"model\"]:\n            model_name = llm_params[\"model\"].split(\"/\")[-1]\n            return handle_model(model_name, \"bedrock\", model_name)\n\n        if \"claude-3-\" in llm_params[\"model\"]:\n            return handle_model(llm_params[\"model\"], \"anthropic\", \"claude3\")\n\n        # Instantiate the language model based on the model name (models that do not use the common interface)\n        if \"deepseek\" in llm_params[\"model\"]:\n            try:\n                self.model_token = models_tokens[\"deepseek\"][llm_params[\"model\"]]\n            except KeyError:\n                print(\"model not found, using default token size (8192)\")\n                self.model_token = 8192\n            return DeepSeek(llm_params)\n\n        if \"ernie\" in llm_params[\"model\"]:\n            try:\n                self.model_token = models_tokens[\"ernie\"][llm_params[\"model\"]]\n            except KeyError:\n                print(\"model not found, using default token size (8192)\")\n                self.model_token = 8192\n            return ErnieBotChat(llm_params)\n        \n        if \"oneapi\" in llm_params[\"model\"]:\n            # take the model after the last dash\n            llm_params[\"model\"] = llm_params[\"model\"].split(\"/\")[-1]\n            try:\n                self.model_token = models_tokens[\"oneapi\"][llm_params[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return OneApi(llm_params)\n        \n        if \"nvidia\" in llm_params[\"model\"]:\n            try:\n                self.model_token = models_tokens[\"nvidia\"][llm_params[\"model\"].split(\"/\")[-1]]\n                llm_params[\"model\"] = \"/\".join(llm_params[\"model\"].split(\"/\")[1:])\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return ChatNVIDIA(llm_params)\n\n        # Raise an error if the model did not match any of the previous cases\n        raise ValueError(\"Model provided by the configuration not supported\")\n\n\n    def get_state(self, key=None) -> dict:\n        \"\"\" \"\"\n        Get the final state of the graph.\n\n        Args:\n            key (str, optional): The key of the final state to retrieve.\n\n        Returns:\n            dict: The final state of the graph.\n        \"\"\"\n\n        if key is not None:\n            return self.final_state[key]\n        return self.final_state\n\n    def append_node(self, node):\n        \"\"\"\n        Add a node to the graph.\n\n        Args:\n            node (BaseNode): The node to add to the graph.\n        \"\"\"\n\n        self.graph.append_node(node)\n\n    def get_execution_info(self):\n        \"\"\"\n        Returns the execution information of the graph.\n\n        Returns:\n            dict: The execution information of the graph.\n        \"\"\"\n\n        return self.execution_info\n\n    @abstractmethod\n    def _create_graph(self):\n        \"\"\"\n        Abstract method to create a graph representation.\n        \"\"\"\n\n    @abstractmethod\n    def run(self) -> str:\n        \"\"\"\n        Abstract method to execute the graph and return the result.\n        \"\"\"\n"
    }
]