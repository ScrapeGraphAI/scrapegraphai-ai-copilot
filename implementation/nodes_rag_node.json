[
    {
        "prompt": "how is rag_node implemented in Scrapegraphai?",
        "answer": "\"\"\"\nRAGNode Module\n\"\"\"\n\nfrom typing import List, Optional\nimport os\n\nfrom langchain.docstore.document import Document\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import (\n    DocumentCompressorPipeline,\n    EmbeddingsFilter,\n)\nfrom langchain_community.document_transformers import EmbeddingsRedundantFilter\nfrom langchain_community.vectorstores import FAISS\n\nfrom langchain_community.chat_models import ChatOllama\nfrom langchain_aws import BedrockEmbeddings, ChatBedrock\nfrom langchain_huggingface import ChatHuggingFace, HuggingFaceEmbeddings\nfrom langchain_community.embeddings import OllamaEmbeddings\nfrom langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\nfrom langchain_google_vertexai import ChatVertexAI, VertexAIEmbeddings\nfrom langchain_fireworks import FireworksEmbeddings, ChatFireworks\nfrom langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings, ChatOpenAI, AzureChatOpenAI\nfrom langchain_nvidia_ai_endpoints import NVIDIAEmbeddings, ChatNVIDIA\n\nfrom ..utils.logging import get_logger\nfrom .base_node import BaseNode\nfrom ..helpers import models_tokens\nfrom ..models import DeepSeek\n\n\nclass RAGNode(BaseNode):\n    \"\"\"\n    A node responsible for compressing the input tokens and storing the document\n    in a vector database for retrieval. Relevant chunks are stored in the state.\n\n    It allows scraping of big documents without exceeding the token limit of the language model.\n\n    Attributes:\n        llm_model: An instance of a language model client, configured for generating answers.\n        embedder_model: An instance of an embedding model client, configured for generating embeddings.\n        verbose (bool): A flag indicating whether to show print statements during execution.\n\n    Args:\n        input (str): Boolean expression defining the input keys needed from the state.\n        output (List[str]): List of output keys to be updated in the state.\n        node_config (dict): Additional configuration for the node.\n        node_name (str): The unique identifier name for the node, defaulting to \"Parse\".\n    \"\"\"\n\n    def __init__(\n        self,\n        input: str,\n        output: List[str],\n        node_config: Optional[dict] = None,\n        node_name: str = \"RAG\",\n    ):\n        super().__init__(node_name, \"node\", input, output, 2, node_config)\n\n        self.llm_model = node_config[\"llm_model\"]\n        self.embedder_model = node_config.get(\"embedder_model\", None)\n        self.verbose = (\n            False if node_config is None else node_config.get(\"verbose\", False)\n        )\n        self.cache_path = node_config.get(\"cache_path\", False)\n\n    def execute(self, state: dict) -> dict:\n        \"\"\"\n        Executes the node's logic to implement RAG (Retrieval-Augmented Generation).\n        The method updates the state with relevant chunks of the document.\n\n        Args:\n            state (dict): The current state of the graph. The input keys will be used to fetch the\n                            correct data from the state.\n\n        Returns:\n            dict: The updated state with the output key containing the relevant chunks of the document.\n\n        Raises:\n            KeyError: If the input keys are not found in the state, indicating that the\n                        necessary information for compressing the content is missing.\n        \"\"\"\n\n        self.logger.info(f\"--- Executing {self.node_name} Node ---\")\n\n        # Interpret input keys based on the provided input expression\n        input_keys = self.get_input_keys(state)\n\n        # Fetching data from the state based on the input keys\n        input_data = [state[key] for key in input_keys]\n\n        user_prompt = input_data[0]\n        doc = input_data[1]\n\n        chunked_docs = []\n\n        for i, chunk in enumerate(doc):\n            doc = Document(\n                page_content=chunk,\n                metadata={\n                    \"chunk\": i + 1,\n                },\n            )\n            chunked_docs.append(doc)\n\n        self.logger.info(\"--- (updated chunks metadata) ---\")\n\n        # check if embedder_model is provided, if not use llm_model\n        if self.embedder_model is not None:\n            embeddings = self.embedder_model\n        elif 'embeddings' in self.node_config:\n            try:\n                embeddings = self._create_embedder(self.node_config['embedder_config'])\n            except Exception:\n                try:\n                    embeddings = self._create_default_embedder()\n                    self.embedder_model = embeddings\n                except ValueError:\n                    embeddings = self.llm_model\n                    self.embedder_model = self.llm_model\n        else:\n            embeddings = self.llm_model\n            self.embedder_model = self.llm_model\n\n        folder_name = self.node_config.get(\"cache_path\", \"cache\")\n\n        if self.node_config.get(\"cache_path\", False) and not os.path.exists(folder_name):\n            index = FAISS.from_documents(chunked_docs, embeddings)\n            os.makedirs(folder_name)\n            index.save_local(folder_name)\n            self.logger.info(\"--- (indexes saved to cache) ---\")\n\n        elif self.node_config.get(\"cache_path\", False) and os.path.exists(folder_name):\n            index = FAISS.load_local(folder_path=folder_name,\n                                     embeddings=embeddings,\n                                     allow_dangerous_deserialization=True)\n            self.logger.info(\"--- (indexes loaded from cache) ---\")\n\n        else:\n            index = FAISS.from_documents(chunked_docs, embeddings)\n\n        retriever = index.as_retriever()\n\n        redundant_filter = EmbeddingsRedundantFilter(embeddings=embeddings)\n        # similarity_threshold could be set, now k=20\n        relevant_filter = EmbeddingsFilter(embeddings=embeddings)\n        pipeline_compressor = DocumentCompressorPipeline(\n            transformers=[redundant_filter, relevant_filter]\n        )\n        # redundant + relevant filter compressor\n        compression_retriever = ContextualCompressionRetriever(\n            base_compressor=pipeline_compressor, base_retriever=retriever\n        )\n\n        # relevant filter compressor only\n        # compression_retriever = ContextualCompressionRetriever(\n        #     base_compressor=relevant_filter, base_retriever=retriever\n        # )\n\n        compressed_docs = compression_retriever.invoke(user_prompt)\n\n        self.logger.info(\"--- (tokens compressed and vector stored) ---\")\n\n        state.update({self.output[0]: compressed_docs})\n        return state\n    \n\n    def _create_default_embedder(self, llm_config=None) -> object:\n        \"\"\"\n        Create an embedding model instance based on the chosen llm model.\n\n        Returns:\n            object: An instance of the embedding model client.\n\n        Raises:\n            ValueError: If the model is not supported.\n        \"\"\"\n        if isinstance(self.llm_model, ChatGoogleGenerativeAI):\n            return GoogleGenerativeAIEmbeddings(\n                google_api_key=llm_config[\"api_key\"], model=\"models/embedding-001\"\n            )\n        if isinstance(self.llm_model, ChatOpenAI):\n            return OpenAIEmbeddings(api_key=self.llm_model.openai_api_key,\n                                    base_url=self.llm_model.openai_api_base)\n        elif isinstance(self.llm_model, DeepSeek):\n            return OpenAIEmbeddings(api_key=self.llm_model.openai_api_key)\n        elif isinstance(self.llm_model, ChatVertexAI):\n            return VertexAIEmbeddings()\n        elif isinstance(self.llm_model, AzureOpenAIEmbeddings):\n            return self.llm_model\n        elif isinstance(self.llm_model, AzureChatOpenAI):\n            return AzureOpenAIEmbeddings()\n        elif isinstance(self.llm_model, ChatFireworks):\n            return FireworksEmbeddings(model=self.llm_model.model_name)\n        elif isinstance(self.llm_model, ChatNVIDIA):\n            return NVIDIAEmbeddings(model=self.llm_model.model_name)\n        elif isinstance(self.llm_model, ChatOllama):\n            # unwrap the kwargs from the model whihc is a dict\n            params = self.llm_model._lc_kwargs\n            # remove streaming and temperature\n            params.pop(\"streaming\", None)\n            params.pop(\"temperature\", None)\n\n            return OllamaEmbeddings(**params)\n        elif isinstance(self.llm_model, ChatHuggingFace):\n            return HuggingFaceEmbeddings(model=self.llm_model.model)\n        elif isinstance(self.llm_model, ChatBedrock):\n            return BedrockEmbeddings(client=None, model_id=self.llm_model.model_id)\n        else:\n            raise ValueError(\"Embedding Model missing or not supported\")\n\n\n    def _create_embedder(self, embedder_config: dict) -> object:\n        \"\"\"\n        Create an embedding model instance based on the configuration provided.\n\n        Args:\n            embedder_config (dict): Configuration parameters for the embedding model.\n\n        Returns:\n            object: An instance of the embedding model client.\n\n        Raises:\n            KeyError: If the model is not supported.\n        \"\"\"\n        embedder_params = {**embedder_config}\n        if \"model_instance\" in embedder_config:\n            return embedder_params[\"model_instance\"]\n        # Instantiate the embedding model based on the model name\n        if \"openai\" in embedder_params[\"model\"]:\n            return OpenAIEmbeddings(api_key=embedder_params[\"api_key\"])\n        if \"azure\" in embedder_params[\"model\"]:\n            return AzureOpenAIEmbeddings()\n        if \"nvidia\" in embedder_params[\"model\"]:\n            embedder_params[\"model\"] = \"/\".join(embedder_params[\"model\"].split(\"/\")[1:])\n            try:\n                models_tokens[\"nvidia\"][embedder_params[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return NVIDIAEmbeddings(model=embedder_params[\"model\"],\n                                    nvidia_api_key=embedder_params[\"api_key\"])\n        if \"ollama\" in embedder_params[\"model\"]:\n            embedder_params[\"model\"] = \"/\".join(embedder_params[\"model\"].split(\"/\")[1:])\n            try:\n                models_tokens[\"ollama\"][embedder_params[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return OllamaEmbeddings(**embedder_params)\n        if \"hugging_face\" in embedder_params[\"model\"]:\n            embedder_params[\"model\"] = \"/\".join(embedder_params[\"model\"].split(\"/\")[1:])\n            try:\n                models_tokens[\"hugging_face\"][embedder_params[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return HuggingFaceEmbeddings(model=embedder_params[\"model\"])\n        if \"fireworks\" in embedder_params[\"model\"]:\n            embedder_params[\"model\"] = \"/\".join(embedder_params[\"model\"].split(\"/\")[1:])\n            try:\n                models_tokens[\"fireworks\"][embedder_params[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return FireworksEmbeddings(model=embedder_params[\"model\"])\n        if \"gemini\" in embedder_params[\"model\"]:\n            try:\n                models_tokens[\"gemini\"][embedder_params[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return GoogleGenerativeAIEmbeddings(model=embedder_params[\"model\"])\n        if \"bedrock\" in embedder_params[\"model\"]:\n            embedder_params[\"model\"] = embedder_params[\"model\"].split(\"/\")[-1]\n            client = embedder_params.get(\"client\", None)\n            try:\n                models_tokens[\"bedrock\"][embedder_params[\"model\"]]\n            except KeyError as exc:\n                raise KeyError(\"Model not supported\") from exc\n            return BedrockEmbeddings(client=client, model_id=embedder_params[\"model\"])\n\n        raise ValueError(\"Model provided by the configuration not supported\")\n"
    }
]